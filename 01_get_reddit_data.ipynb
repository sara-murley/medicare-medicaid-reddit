{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0521914e-25a6-41a1-9b84-5ee9981f9b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Collecting up to 1000 posts from r/Medicare...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 997/997 [1:41:20<00:00,  6.10s/it]\n",
      " 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                          | 361/997 [09:22<16:30,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Collecting up to 1000 posts from r/Medicaid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 988/988 [30:38<00:00,  1.86s/it]\n",
      " 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                 | 184/988 [05:38<24:40,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Collecting up to 1000 posts from r/HealthInsurance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [03:53<00:00,  4.29it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [03:51<00:00,  4.32it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [03:52<00:00,  4.30it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [03:53<00:00,  4.28it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [17:55<00:00,  1.08s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [03:58<00:00,  4.19it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [03:57<00:00,  4.20it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [03:57<00:00,  4.21it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 999/999 [04:26<00:00,  3.75it/s]\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                | 198/999 [01:00<04:03,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Done! Collected 1698 posts and 4151 comments across 3 subreddits.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import prawcore\n",
    "\n",
    "# --- Reddit API credentials ---\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"KM3SXXKIOVv0T2Q7YIDO-g\",\n",
    "    client_secret=\"P5eVSfGwsouYLMYfXEe1GoMSJLj4jA\",\n",
    "    user_agent=\"healthcare-analysis by u/slmdatascience\"\n",
    ")\n",
    "\n",
    "# --- Parameters ---\n",
    "subreddits = [\"Medicare\", \"Medicaid\", \"HealthInsurance\"]\n",
    "keywords = [\"medicare\", \"medicaid\"]\n",
    "max_posts = 1000         # total posts per subreddit\n",
    "comments_per_post = 3    # top comments per post\n",
    "sleep_between_posts = 2  # seconds\n",
    "\n",
    "# --- Storage ---\n",
    "posts = []\n",
    "comments = []\n",
    "\n",
    "# --- Helper function to fetch top comments safely ---\n",
    "def fetch_top_comments(submission, top_n):\n",
    "    try:\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        return submission.comments[:top_n]\n",
    "    except prawcore.exceptions.TooManyRequests:\n",
    "        print(\"Rate limit hit while fetching comments. Sleeping 60 seconds...\")\n",
    "        time.sleep(60)\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        return submission.comments[:top_n]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching comments for post {submission.id}: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Scrape each subreddit ---\n",
    "for sub in subreddits:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    collected = 0\n",
    "    last_timestamp = None\n",
    "\n",
    "    print(f\"\\nüîç Collecting up to {max_posts} posts from r/{sub}...\")\n",
    "\n",
    "    while collected < max_posts:\n",
    "        params = {}\n",
    "        if last_timestamp:\n",
    "            params[\"before\"] = int(last_timestamp)\n",
    "\n",
    "        try:\n",
    "            batch = list(subreddit.new(limit=1000, params=params))\n",
    "        except prawcore.exceptions.TooManyRequests:\n",
    "            print(\"Rate limit hit while fetching posts. Sleeping 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching posts: {e}\")\n",
    "            break\n",
    "\n",
    "        if not batch:\n",
    "            print(\"No more posts available in this subreddit.\")\n",
    "            break\n",
    "\n",
    "        for submission in tqdm(batch):\n",
    "            last_timestamp = submission.created_utc\n",
    "\n",
    "            text = (submission.title + \" \" + submission.selftext).lower()\n",
    "            if not any(k in text for k in keywords):\n",
    "                continue\n",
    "\n",
    "            posts.append({\n",
    "                \"id\": submission.id,\n",
    "                \"title\": submission.title,\n",
    "                \"selftext\": submission.selftext,\n",
    "                \"subreddit\": sub,\n",
    "                \"score\": submission.score,\n",
    "                \"num_comments\": submission.num_comments,\n",
    "                \"created_utc\": submission.created_utc,\n",
    "                \"date\": datetime.fromtimestamp(submission.created_utc),\n",
    "                \"url\": submission.url\n",
    "            })\n",
    "\n",
    "            # Top comments\n",
    "            top_comments = fetch_top_comments(submission, comments_per_post)\n",
    "            for c in top_comments:\n",
    "                comments.append({\n",
    "                    \"post_id\": submission.id,\n",
    "                    \"comment_id\": c.id,\n",
    "                    \"body\": c.body,\n",
    "                    \"score\": c.score,\n",
    "                    \"created_utc\": c.created_utc,\n",
    "                    \"date\": datetime.fromtimestamp(c.created_utc)\n",
    "                })\n",
    "\n",
    "            collected += 1\n",
    "            if collected >= max_posts:\n",
    "                break\n",
    "\n",
    "            time.sleep(sleep_between_posts)\n",
    "\n",
    "# --- Create DataFrames and save ---\n",
    "df_posts = pd.DataFrame(posts).drop_duplicates(subset=[\"id\"])\n",
    "df_comments = pd.DataFrame(comments).drop_duplicates(subset=[\"comment_id\"])\n",
    "\n",
    "df_posts.to_csv(\"reddit_posts_2.csv\", index=False)\n",
    "df_comments.to_csv(\"reddit_comments_2.csv\", index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Done! Collected {len(df_posts)} posts and {len(df_comments)} comments across {len(subreddits)} subreddits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031a322-cd95-4f46-86c1-61c8cd84124c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
